<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Random Forest - Machine Learning Cookbook(V2)</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/v4-shims.min.css">
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.8/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: '#3B82F6',
                        secondary: '#8B5CF6',
                        accent: '#10B981',
                        neutral: '#6B7280',
                        dark: '#1F2937',
                        light: '#F9FAFB'
                    },
                    fontFamily: {
                        code: ['Consolas', 'Monaco', 'monospace'],
                        sans: ['Inter', 'system-ui', 'sans-serif']
                    }
                }
            }
        }
    </script>
    
    <style type="text/tailwindcss">
        @layer utilities {
            .content-auto {
                content-visibility: auto;
            }
            .math-card {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            }
            .code-card {
                background: linear-gradient(135deg, #1f2937 0%, #374151 100%);
            }
            .interactive-card {
                background: linear-gradient(135deg, #10b981 0%, #059669 100%);
            }
            .formula-highlight {
                background: linear-gradient(45deg, rgba(59, 130, 246, 0.1), rgba(139, 92, 246, 0.1));
                border-left: 4px solid #8B5CF6;
            }
        }
    </style>
</head>
<body class="bg-gray-50">
    <div class="bg-white rounded-xl shadow-lg p-6">
        <!-- Algorithm Header -->
        <div class="mb-8">
            <div class="flex items-center mb-4">
                <i class="fa fa-tree text-3xl text-purple-600 mr-4"></i>
                <div>
                    <h1 class="text-3xl font-bold text-gray-800">Random Forest</h1>
                    <p class="text-gray-600">Ensemble Learning - Classification/Regression Algorithm</p>
                </div>
            </div>
            <div class="flex flex-wrap gap-2 mb-4">
                <span class="bg-purple-100 text-purple-800 px-3 py-1 rounded-full text-sm">Ensemble</span>
                <span class="bg-green-100 text-green-800 px-3 py-1 rounded-full text-sm">Bagging</span>
                <span class="bg-blue-100 text-blue-800 px-3 py-1 rounded-full text-sm">Tree-based</span>
                <span class="bg-yellow-100 text-yellow-800 px-3 py-1 rounded-full text-sm">Robust</span>
            </div>
        </div>

        <!-- Algorithm Overview -->
        <div class="mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">Algorithm Overview</h2>
            <div class="bg-gray-50 p-6 rounded-lg">
                <p class="text-gray-700 mb-4">
                    Random Forest is an ensemble learning method that constructs multiple decision trees during training 
                    and outputs the class that is the mode of the classes (classification) or mean prediction (regression) 
                    of the individual trees. It combines bagging (bootstrap aggregation) with random feature selection.
                </p>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                    <div>
                        <h3 class="font-bold text-gray-800 mb-2">Key Characteristics</h3>
                        <ul class="list-disc list-inside text-gray-700 space-y-1">
                            <li>Ensemble of decision trees</li>
                            <li>Uses bootstrap sampling (bagging)</li>
                            <li>Random feature subset selection</li>
                            <li>Reduces variance through averaging</li>
                            <li>Handles high dimensional data well</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="font-bold text-gray-800 mb-2">Applications</h3>
                        <ul class="list-disc list-inside text-gray-700 space-y-1">
                            <li>Credit scoring</li>
                            <li>Medical diagnosis</li>
                            <li>Customer churn prediction</li>
                            <li>Fraud detection</li>
                            <li>Image classification</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>

        <!-- Mathematical Foundations -->
        <div class="mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">Mathematical Foundations</h2>
            
            <!-- Ensemble Principle -->
            <div class="mb-6">
                <h3 class="text-xl font-bold text-gray-800 mb-3">1. Ensemble Principle</h3>
                <div class="formula-highlight p-4 rounded-lg mb-4">
                    <p class="text-lg font-mono">\[ \hat{y} = \frac{1}{M} \sum_{m=1}^{M} \hat{y}_m(x) \]</p>
                    <p class="text-gray-600 text-sm mt-2">
                        For regression: average of individual tree predictions<br>
                        For classification: majority vote of individual tree predictions
                    </p>
                </div>
            </div>

            <!-- Bootstrap Sampling -->
            <div class="mb-6">
                <h3 class="text-xl font-bold text-gray-800 mb-3">2. Bootstrap Sampling</h3>
                <div class="formula-highlight p-4 rounded-lg mb-4">
                    <p class="text-lg font-mono">\[ \text{Bootstrap sample: } \mathcal{D}_m = \{x_{i_1}, x_{i_2}, \dots, x_{i_n}\} \quad i_j \sim \text{Uniform}(1, n) \]</p>
                    <p class="text-gray-600 text-sm mt-2">
                        Each tree is trained on a random sample with replacement<br>
                        Out-of-bag (OOB) samples: ~36.8% of original data not in bootstrap sample
                    </p>
                </div>
            </div>

            <!-- Random Feature Selection -->
            <div class="mb-6">
                <h3 class="text-xl font-bold text-gray-800 mb-3">3. Random Feature Selection</h3>
                <div class="formula-highlight p-4 rounded-lg mb-4">
                    <p class="text-lg font-mono">\[ k = \sqrt{d} \quad \text{or} \quad k = \frac{d}{3} \]</p>
                    <p class="text-gray-600 text-sm mt-2">
                        At each split, only k random features are considered<br>
                        d is the total number of features
                    </p>
                </div>
            </div>

            <!-- Variance Reduction -->
            <div class="mb-6">
                <h3 class="text-xl font-bold text-gray-800 mb-3">4. Variance Reduction</h3>
                <div class="formula-highlight p-4 rounded-lg mb-4">
                    <p class="text-lg font-mono">\[ \text{Var}(\hat{y}) = \frac{\text{Var}(\hat{y}_m)}{M} + \left(1 - \frac{1}{M}\right) \text{Cov}(\hat{y}_m, \hat{y}_l) \]</p>
                    <p class="text-gray-600 text-sm mt-2">
                        Variance decreases as M increases and covariance decreases<br>
                        Random feature selection reduces correlation between trees
                    </p>
                </div>
                
                <!-- Variance Reduction Visualization -->
                <div class="bg-white p-4 rounded-lg shadow-md">
                    <canvas id="varianceChart" width="400" height="200"></canvas>
                </div>
            </div>
        </div>

        <!-- Implementation -->
        <div class="mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">Implementation</h2>
            
            <!-- Python Code -->
            <div class="code-card text-white p-6 rounded-lg mb-6">
                <pre class="font-code text-sm overflow-x-auto"><code>import numpy as np
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.datasets import load_iris, load_boston
import matplotlib.pyplot as plt

# Classification example with Iris dataset
iris = load_iris()
X_clf, y_clf = iris.data, iris.target
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X_clf, y_clf, test_size=0.3, random_state=42
)

# Create and train Random Forest classifier
clf_rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    bootstrap=True,
    oob_score=True,
    random_state=42
)
clf_rf.fit(X_train_clf, y_train_clf)

# Make predictions
y_pred_clf = clf_rf.predict(X_test_clf)
accuracy = accuracy_score(y_test_clf, y_pred_clf)
print(f"Classification Accuracy: {accuracy:.2f}")
print(f"OOB Score: {clf_rf.oob_score_:.2f}")

# Regression example with Boston housing data
boston = load_boston()
X_reg, y_reg = boston.data, boston.target
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42
)

# Create and train Random Forest regressor
reg_rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    bootstrap=True,
    oob_score=True,
    random_state=42
)
reg_rf.fit(X_train_reg, y_train_reg)

# Make predictions
y_pred_reg = reg_rf.predict(X_test_reg)
mse = mean_squared_error(y_test_reg, y_pred_reg)
print(f"\nRegression MSE: {mse:.2f}")
print(f"OOB Score (R²): {reg_rf.oob_score_:.2f}")

# Feature importance
print("\nFeature Importance (Classification):")
for name, importance in zip(iris.feature_names, clf_rf.feature_importances_):
    print(f"{name}: {importance:.4f}")

print("\nFeature Importance (Regression):")
for name, importance in zip(boston.feature_names, reg_rf.feature_importances_):
    print(f"{name}: {importance:.4f}")

# Plot feature importance
plt.figure(figsize=(10, 6))
plt.barh(boston.feature_names, reg_rf.feature_importances_)
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importance (Boston Housing)')
plt.tight_layout()
plt.show()

# Effect of number of trees on performance
def plot_trees_effect(X_train, X_test, y_train, y_test):
    n_estimators_range = [1, 5, 10, 20, 50, 100, 200, 500]
    train_scores = []
    test_scores = []
    
    for n in n_estimators_range:
        model = RandomForestClassifier(n_estimators=n, random_state=42)
        model.fit(X_train, y_train)
        train_scores.append(accuracy_score(y_train, model.predict(X_train)))
        test_scores.append(accuracy_score(y_test, model.predict(X_test)))
    
    plt.figure(figsize=(10, 6))
    plt.plot(n_estimators_range, train_scores, 'o-', label='Training Accuracy')
    plt.plot(n_estimators_range, test_scores, 'o-', label='Test Accuracy')
    plt.xlabel('Number of Trees')
    plt.ylabel('Accuracy')
    plt.title('Effect of Number of Trees on Performance')
    plt.legend()
    plt.show()

plot_trees_effect(X_train_clf, X_test_clf, y_train_clf, y_test_clf)</code></pre>
            </div>

            <!-- Interactive Demo -->
            <div class="interactive-card text-white p-6 rounded-lg">
                <h3 class="text-xl font-bold mb-4">Interactive Random Forest Demo</h3>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                    <div>
                        <label class="block mb-2 font-bold">Number of Trees:</label>
                        <input type="number" id="numTrees" value="100" min="10" max="500" step="10" class="w-full p-2 rounded-lg mb-4 text-gray-800">
                        
                        <label class="block mb-2 font-bold">Max Depth:</label>
                        <input type="number" id="maxDepth" value="5" min="1" max="20" class="w-full p-2 rounded-lg mb-4 text-gray-800">
                        
                        <label class="block mb-2 font-bold">Max Features:</label>
                        <select id="maxFeatures" class="w-full p-2 rounded-lg mb-4 text-gray-800">
                            <option value="sqrt">sqrt</option>
                            <option value="log2">log2</option>
                            <option value="None">None (all features)</option>
                        </select>
                        
                        <button onclick="runRandomForest()" class="bg-white text-green-600 px-4 py-2 rounded-lg hover:bg-gray-100 transition-colors duration-200 font-bold">
                            Run Random Forest
                        </button>
                    </div>
                    <div>
                        <canvas id="rfPerformanceChart" width="400" height="300" style="background-color: white;"></canvas>
                    </div>
                </div>
            </div>
        </div>

        <!-- Mathematical Theory Card -->
        <div class="math-card text-white p-6 rounded-lg mb-8">
            <h3 class="text-xl font-bold mb-4">Mathematical Theory</h3>
            <div class="space-y-4">
                <div>
                    <h4 class="font-bold mb-2">Hoeffding's Inequality</h4>
                    <p class="text-sm">Provides theoretical guarantees for the generalization error of ensemble methods:</p>
                    <p class="text-sm mt-1 font-mono">\[ P(|\hat{\mu} - \mu| > \epsilon) \leq 2e^{-2n\epsilon^2} \]</p>
                    <p class="text-sm">Where \( \hat{\mu} \) is the sample mean and \( \mu \) is the true mean.</p>
                </div>
                <div>
                    <h4 class="font-bold mb-2">Bias-Variance Decomposition</h4>
                    <p class="text-sm">Random Forest reduces variance while maintaining low bias:</p>
                    <ul class="list-disc list-inside text-xs mt-1">
                        <li><span class="font-bold">Bias:</span> Similar to individual trees, depends on tree depth</li>
                        <li><span class="font-bold">Variance:</span> Reduced by averaging independent trees</li>
                        <li><span class="font-bold">Total Error:</span> Decreases as more trees are added</li>
                    </ul>
                </div>
                <div>
                    <h4 class="font-bold mb-2">OOB Error Estimation</h4>
                    <p class="text-sm">Out-of-bag error provides an unbiased estimate of the generalization error without needing a separate validation set:</p>
                    <p class="text-sm mt-1 font-mono">\[ \text{OOB Error} = \frac{1}{n} \sum_{i=1}^{n} I(y_i \neq \hat{y}_i^{\text{OOB}}) \]</p>
                </div>
                <div>
                    <h4 class="font-bold mb-2">Feature Importance</h4>
                    <p class="text-sm">Calculated based on the decrease in node impurity weighted by the probability of reaching that node:</p>
                    <p class="text-sm mt-1 font-mono">\[ \text{Importance}(j) = \frac{1}{M} \sum_{m=1}^{M} \sum_{t \in T_m} \frac{n_t}{n} \Delta I_t(j) \]</p>
                </div>
                <div>
                    <h4 class="font-bold mb-2">Computational Complexity</h4>
                    <p class="text-sm">Training time complexity is \( O(M n d \log n) \) where:</p>
                    <ul class="list-disc list-inside text-xs mt-1">
                        <li>M = number of trees</li>
                        <li>n = number of samples</li>
                        <li>d = number of features</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Evaluation Metrics -->
        <div class="mb-8">
            <h2 class="text-2xl font-bold text-gray-800 mb-4">Evaluation Metrics</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-4">
                <div class="bg-white p-4 rounded-lg shadow-md text-center">
                    <i class="fa fa-check-circle text-2xl text-green-500 mb-2"></i>
                    <h3 class="font-bold text-gray-800 mb-1">Accuracy</h3>
                    <p class="text-sm text-gray-600">Overall correctness (classification)</p>
                </div>
                <div class="bg-white p-4 rounded-lg shadow-md text-center">
                    <i class="fa fa-square text-2xl text-red-500 mb-2"></i>
                    <h3 class="font-bold text-gray-800 mb-1">MSE</h3>
                    <p class="text-sm text-gray-600">Mean Squared Error (regression)</p>
                </div>
                <div class="bg-white p-4 rounded-lg shadow-md text-center">
                    <i class="fa fa-balance-scale text-2xl text-blue-500 mb-2"></i>
                    <h3 class="font-bold text-gray-800 mb-1">OOB Score</h3>
                    <p class="text-sm text-gray-600">Out-of-bag error estimate</p>
                </div>
                <div class="bg-white p-4 rounded-lg shadow-md text-center">
                    <i class="fa fa-star text-2xl text-yellow-500 mb-2"></i>
                    <h3 class="font-bold text-gray-800 mb-1">R² Score</h3>
                    <p class="text-sm text-gray-600">Coefficient of determination</p>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Global chart variables
        let rfPerformanceChartInstance = null;
        let varianceChartInstance = null;
        
        // Variance reduction visualization
        function plotVarianceReduction() {
            const ctx = document.getElementById('varianceChart').getContext('2d');
            
            // Destroy previous chart instance if it exists
            if (varianceChartInstance) {
                varianceChartInstance.destroy();
            }
            
            // Sample data: variance vs number of trees
            const nTrees = [1, 5, 10, 20, 50, 100, 200, 500];
            const variance = [1.0, 0.6, 0.45, 0.35, 0.25, 0.20, 0.18, 0.17];
            
            varianceChartInstance = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: nTrees,
                    datasets: [{
                        label: 'Variance',
                        data: variance,
                        borderColor: '#8B5CF6',
                        backgroundColor: 'rgba(139, 92, 246, 0.1)',
                        borderWidth: 3,
                        pointBackgroundColor: '#8B5CF6',
                        pointRadius: 5,
                        fill: true,
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    backgroundColor: '#FFFFFF',
                    scales: {
                        x: { title: { display: true, text: 'Number of Trees' } },
                        y: { title: { display: true, text: 'Variance' }, min: 0, max: 1.1 }
                    },
                    plugins: {
                        title: { display: true, text: 'Variance Reduction with More Trees' }
                    }
                }
            });
        }

        // Interactive Random Forest demo
        function runRandomForest() {
            const nTrees = parseInt(document.getElementById('numTrees').value);
            const maxDepth = parseInt(document.getElementById('maxDepth').value);
            const maxFeatures = document.getElementById('maxFeatures').value;
            
            // Generate sample performance data
            const performanceData = generatePerformanceData(nTrees, maxDepth, maxFeatures);
            
            // Plot results
            plotRFPerformance(performanceData);
        }

        function generatePerformanceData(nTrees, maxDepth, maxFeatures) {
            // Simulate performance based on parameters
            const nTreesRange = [10, 20, 50, 100, 200, 300, 400, 500];
            const trainAccuracy = nTreesRange.map(n => {
                const base = 0.95 + (maxDepth > 10 ? 0.03 : 0);
                const decay = Math.exp(-(500 - n) / 200);
                return base * decay + 0.05 * Math.random();
            });
            
            const testAccuracy = nTreesRange.map(n => {
                const base = 0.85 + (maxDepth > 10 ? -0.02 : 0.03);
                const improvement = 1 - Math.exp(-n / 100);
                return base * improvement + 0.1 * Math.random();
            });
            
            return { nTreesRange, trainAccuracy, testAccuracy };
        }

        function plotRFPerformance(data) {
            const ctx = document.getElementById('rfPerformanceChart').getContext('2d');
            
            // Destroy previous chart instance if it exists
            if (rfPerformanceChartInstance) {
                rfPerformanceChartInstance.destroy();
            }
            
            rfPerformanceChartInstance = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: data.nTreesRange,
                    datasets: [
                        {
                            label: 'Training Accuracy',
                            data: data.trainAccuracy,
                            borderColor: '#10B981',
                            backgroundColor: 'rgba(16, 185, 129, 0.1)',
                            borderWidth: 3,
                            pointBackgroundColor: '#10B981',
                            pointRadius: 5,
                            fill: true,
                            tension: 0.1
                        },
                        {
                            label: 'Test Accuracy',
                            data: data.testAccuracy,
                            borderColor: '#3B82F6',
                            backgroundColor: 'rgba(59, 130, 246, 0.1)',
                            borderWidth: 3,
                            pointBackgroundColor: '#3B82F6',
                            pointRadius: 5,
                            fill: true,
                            tension: 0.1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    backgroundColor: '#FFFFFF',
                    scales: {
                        x: { title: { display: true, text: 'Number of Trees' } },
                        y: { title: { display: true, text: 'Accuracy' }, min: 0.5, max: 1.0 }
                    },
                    plugins: {
                        title: { display: true, text: 'Random Forest Performance' }
                    }
                }
            });
        }

        // Initialize charts when page loads
        document.addEventListener('DOMContentLoaded', function() {
            plotVarianceReduction();
            runRandomForest(); // Run initial demo
        });
    </script>
</body>
</html>